{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T02:46:40.018178Z",
     "start_time": "2020-06-08T02:46:40.015413Z"
    }
   },
   "source": [
    "# Preprocess\n",
    "\n",
    "1. Preproess content\n",
    "2. Convert .json to data frames\n",
    "3. Get features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wPeSDTc2EnDG"
   },
   "source": [
    "## Preprocess JSON data\n",
    "\n",
    "1.   Statistics\n",
    "2.   Convert to pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T12:39:25.313720Z",
     "start_time": "2020-06-08T12:39:24.232764Z"
    },
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-16T02:16:29.621737Z",
     "iopub.status.busy": "2020-06-16T02:16:29.621517Z",
     "iopub.status.idle": "2020-06-16T02:16:30.492400Z",
     "shell.execute_reply": "2020-06-16T02:16:30.491910Z",
     "shell.execute_reply.started": "2020-06-16T02:16:29.621716Z"
    },
    "id": "7dGi8azrCjMN"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Load raw data in .json format\n",
    "'''\n",
    "import json\n",
    "\n",
    "document_map = json.load(open('./data/raw/documents.json'))\n",
    "training_map = json.load(open('./data/raw/trainingset.json'))\n",
    "validation_map = json.load(open('./data/raw/validationset.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T02:21:48.914447Z",
     "start_time": "2020-06-08T02:21:48.905241Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 896,
     "status": "ok",
     "timestamp": 1590680901424,
     "user": {
      "displayName": "G DC",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgdTpZxzZ2QbpOVgSBCa49qEewNSQ8rsi8Y9A_W=s64",
      "userId": "09626979524344434372"
     },
     "user_tz": -480
    },
    "id": "5vM98Ar_7Gvv",
    "outputId": "b21a704b-f253-43f4-af3f-c878d87372b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "186\n"
     ]
    }
   ],
   "source": [
    "# Maximum length of single query\n",
    "query_len_list = []\n",
    "for (key, value) in training_map['queries'].items():\n",
    "  query_len_list.append(len(value))\n",
    "print(len(query_len_list))\n",
    "print(max(query_len_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T02:24:23.781078Z",
     "start_time": "2020-06-08T02:24:23.636616Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1008,
     "status": "ok",
     "timestamp": 1590680891504,
     "user": {
      "displayName": "G DC",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgdTpZxzZ2QbpOVgSBCa49qEewNSQ8rsi8Y9A_W=s64",
      "userId": "09626979524344434372"
     },
     "user_tz": -480
    },
    "id": "d9GOk7Q9Ey6U",
    "outputId": "97ecae85-3ead-4c9a-c326-4ed4809a5a44"
   },
   "outputs": [],
   "source": [
    "# Maximum length of single document\n",
    "doc_len_list = []\n",
    "for (key, value) in document_map.items():\n",
    "  doc_len_list.append(len(value))\n",
    "print(len(doc_len_list))\n",
    "print(max(doc_len_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T02:26:42.864787Z",
     "start_time": "2020-06-08T02:26:38.431065Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-16T02:16:36.886468Z",
     "iopub.status.busy": "2020-06-16T02:16:36.886205Z",
     "iopub.status.idle": "2020-06-16T02:16:37.441635Z",
     "shell.execute_reply": "2020-06-16T02:16:37.441132Z",
     "shell.execute_reply.started": "2020-06-16T02:16:36.886443Z"
    },
    "executionInfo": {
     "elapsed": 1010,
     "status": "ok",
     "timestamp": 1591196060502,
     "user": {
      "displayName": "G DC",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgdTpZxzZ2QbpOVgSBCa49qEewNSQ8rsi8Y9A_W=s64",
      "userId": "09626979524344434372"
     },
     "user_tz": -480
    },
    "id": "jkx__3fEgb0Q",
    "outputId": "ec429735-ec2b-4b4b-c713-4f706c3056b3"
   },
   "outputs": [],
   "source": [
    "# Prepare for preprocessing\n",
    "from preprocess import TextPreprocessor as TPP\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from unidecode import unidecode\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-16T02:21:29.154051Z",
     "iopub.status.busy": "2020-06-16T02:21:29.153823Z",
     "iopub.status.idle": "2020-06-16T02:21:32.324301Z",
     "shell.execute_reply": "2020-06-16T02:21:32.323860Z",
     "shell.execute_reply.started": "2020-06-16T02:21:29.154029Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T02:35:39.527947Z",
     "start_time": "2020-06-08T02:27:21.749205Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-16T02:21:34.090898Z",
     "iopub.status.busy": "2020-06-16T02:21:34.090675Z",
     "iopub.status.idle": "2020-06-16T02:29:49.736774Z",
     "shell.execute_reply": "2020-06-16T02:29:49.736055Z",
     "shell.execute_reply.started": "2020-06-16T02:21:34.090875Z"
    },
    "executionInfo": {
     "elapsed": 5385032,
     "status": "ok",
     "timestamp": 1591201783303,
     "user": {
      "displayName": "G DC",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgdTpZxzZ2QbpOVgSBCa49qEewNSQ8rsi8Y9A_W=s64",
      "userId": "09626979524344434372"
     },
     "user_tz": -480
    },
    "id": "v_OBRgmindwb",
    "outputId": "33c5018e-e467-4c17-cfde-85673a5abf37",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "100%|██████████| 62500/62500 [07:54<00:00, 131.62it/s]\n",
      "100%|██████████| 62500/62500 [07:53<00:00, 131.90it/s]\n",
      "100%|██████████| 62500/62500 [07:55<00:00, 131.41it/s]\n",
      "100%|██████████| 62500/62500 [07:55<00:00, 131.46it/s]\n",
      "100%|██████████| 62500/62500 [07:54<00:00, 131.76it/s]\n",
      "100%|██████████| 62500/62500 [07:55<00:00, 131.51it/s]\n",
      "100%|██████████| 62500/62500 [08:00<00:00, 130.06it/s]\n",
      "100%|██████████| 62500/62500 [08:05<00:00, 128.76it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "  Preprocess documents.json and convert to .csv\n",
    "'''\n",
    "\n",
    "csv_folder = './data/csv/'\n",
    "document_csv_file = csv_folder + 'documents.csv'\n",
    "\n",
    "tpp = TPP()\n",
    "\n",
    "from multiprocessing import Manager, Process, Pool\n",
    "\n",
    "def func(proc_idx, data_list, start_idx, end_idx, ret_dict):\n",
    "    doc_pd = pd.DataFrame({}, columns=['doc_id', 'doc_text'])\n",
    "    pd_idx = 0\n",
    "    for (doc_id, doc_text) in tqdm(data_list[start_idx:end_idx]):\n",
    "        doc_pd.loc[pd_idx, 'doc_id'] = doc_id\n",
    "        doc_pd.loc[pd_idx, 'doc_text'] = ' '.join(tpp.preprocess(unidecode(doc_text)))\n",
    "        pd_idx = pd_idx + 1\n",
    "    ret_dict[proc_idx] = doc_pd\n",
    "    \n",
    "data_list = list(document_map.items())\n",
    "data_list_len = len(data_list)\n",
    "proc_num = 8\n",
    "chunk_size = data_list_len // proc_num\n",
    "\n",
    "pool = Pool(proc_num)\n",
    "manager = Manager()\n",
    "ret_dict = manager.dict()\n",
    "\n",
    "for proc_idx in range(proc_num):\n",
    "    start_idx = proc_idx * chunk_size\n",
    "    end_idx = min(data_list_len, start_idx + chunk_size)\n",
    "    pool.apply_async(func, args=(proc_idx, data_list, start_idx, end_idx, ret_dict))\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "doc_frame = pd.concat([ret_dict[i] for i in range(proc_num)], axis=0)\n",
    "doc_frame.to_csv(document_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T02:37:45.505579Z",
     "start_time": "2020-06-08T02:37:45.500871Z"
    },
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-16T02:38:41.187546Z",
     "iopub.status.busy": "2020-06-16T02:38:41.187303Z",
     "iopub.status.idle": "2020-06-16T02:38:41.192851Z",
     "shell.execute_reply": "2020-06-16T02:38:41.192399Z",
     "shell.execute_reply.started": "2020-06-16T02:38:41.187519Z"
    },
    "id": "KuYO7jlQJGQ0"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Preprocess json dataset with labels and convert to .csv file\n",
    "'''\n",
    "\n",
    "def json2csv(data_map):\n",
    "    # Process query_id and query_text\n",
    "    df = pd.DataFrame({}, columns=['query_id', 'query_text', 'query_label'])\n",
    "    tpp = TPP()\n",
    "    \n",
    "    pd_idx = 0\n",
    "    for (query_id, query_text) in tqdm(data_map['queries'].items()):\n",
    "        df.loc[pd_idx, 'query_id'] = query_id\n",
    "        df.loc[pd_idx, 'query_text'] = ' '.join(tpp.preprocess(unidecode(query_text)))\n",
    "        pd_idx = pd_idx + 1\n",
    "    \n",
    "    pd_idx = 0\n",
    "    for (query_id, query_label) in tqdm(data_map['labels'].items()):\n",
    "        assert str(df.loc[pd_idx, 'query_id']) == query_id\n",
    "        df.loc[pd_idx, 'query_label'] = ' '.join(query_label)\n",
    "        pd_idx = pd_idx + 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T02:39:01.212963Z",
     "start_time": "2020-06-08T02:37:50.357073Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-16T02:38:42.798961Z",
     "iopub.status.busy": "2020-06-16T02:38:42.798717Z",
     "iopub.status.idle": "2020-06-16T02:39:53.594346Z",
     "shell.execute_reply": "2020-06-16T02:39:53.593881Z",
     "shell.execute_reply.started": "2020-06-16T02:38:42.798938Z"
    },
    "executionInfo": {
     "elapsed": 117619,
     "status": "ok",
     "timestamp": 1591196185605,
     "user": {
      "displayName": "G DC",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgdTpZxzZ2QbpOVgSBCa49qEewNSQ8rsi8Y9A_W=s64",
      "userId": "09626979524344434372"
     },
     "user_tz": -480
    },
    "id": "TWeyl_QBza-i",
    "outputId": "07bf54e7-0eea-481f-fa90-6ebb94b3f8f5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "100%|██████████| 30000/30000 [01:01<00:00, 489.88it/s]\n",
      "100%|██████████| 30000/30000 [00:04<00:00, 6033.41it/s]\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "100%|██████████| 3000/3000 [00:04<00:00, 749.41it/s]\n",
      "100%|██████████| 3000/3000 [00:00<00:00, 5939.93it/s]\n"
     ]
    }
   ],
   "source": [
    "csv_folder = './data/csv/'\n",
    "training_csv = csv_folder + 'training.csv'\n",
    "validation_csv = csv_folder + 'validation.csv'\n",
    "\n",
    "json2csv(training_map).to_csv(training_csv)\n",
    "json2csv(validation_map).to_csv(validation_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OgUkM4JgE1jC"
   },
   "source": [
    "## Prepare for Wrod2Vec model\n",
    "\n",
    "1.   Dictionary preparation\n",
    "2.   Corpus preparation\n",
    "3.   TF-IDF preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T02:41:27.520863Z",
     "start_time": "2020-06-08T02:41:27.279520Z"
    },
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-16T02:41:19.065576Z",
     "iopub.status.busy": "2020-06-16T02:41:19.065319Z",
     "iopub.status.idle": "2020-06-16T02:41:19.344664Z",
     "shell.execute_reply": "2020-06-16T02:41:19.344109Z",
     "shell.execute_reply.started": "2020-06-16T02:41:19.065552Z"
    },
    "id": "nhaJnzVUEUPi"
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, similarities, models\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T02:41:36.760599Z",
     "start_time": "2020-06-08T02:41:32.887088Z"
    },
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-16T03:51:31.818598Z",
     "iopub.status.busy": "2020-06-16T03:51:31.818319Z",
     "iopub.status.idle": "2020-06-16T03:51:35.773987Z",
     "shell.execute_reply": "2020-06-16T03:51:35.773371Z",
     "shell.execute_reply.started": "2020-06-16T03:51:31.818573Z"
    },
    "id": "Tla5mN2NEDqX"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Prepare for dictionary, TF-IDF model, sprase matrix\n",
    "'''\n",
    "\n",
    "csv_folder = './data/csv/'\n",
    "document_csv_file = csv_folder + 'documents.csv'\n",
    "document_pd = pd.read_csv(document_csv_file)\n",
    "# pool all text from documents\n",
    "raw_text = document_pd['doc_text'].values.tolist()\n",
    "# pool all words from documents\n",
    "text_pool = [line.split() for line in raw_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T02:41:46.258597Z",
     "start_time": "2020-06-08T02:41:36.762038Z"
    },
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-16T03:51:35.775517Z",
     "iopub.status.busy": "2020-06-16T03:51:35.775148Z",
     "iopub.status.idle": "2020-06-16T03:51:44.649549Z",
     "shell.execute_reply": "2020-06-16T03:51:44.649036Z",
     "shell.execute_reply.started": "2020-06-16T03:51:35.775483Z"
    },
    "id": "0yCKlsDcMaHq"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Clean corpus\n",
    "'''\n",
    "from collections import defaultdict\n",
    "# Remove words appear once\n",
    "word_freq = defaultdict(int)\n",
    "for line in text_pool:\n",
    "    for word in line:\n",
    "        word_freq[word] += 1\n",
    "text_pool = [[token for token in line if word_freq[token] > 1] for line in text_pool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T02:45:07.808873Z",
     "start_time": "2020-06-08T02:43:12.082699Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-16T03:53:45.557118Z",
     "iopub.status.busy": "2020-06-16T03:53:45.556883Z",
     "iopub.status.idle": "2020-06-16T03:55:40.732560Z",
     "shell.execute_reply": "2020-06-16T03:55:40.732068Z",
     "shell.execute_reply.started": "2020-06-16T03:53:45.557096Z"
    },
    "executionInfo": {
     "elapsed": 168242,
     "status": "ok",
     "timestamp": 1591230654630,
     "user": {
      "displayName": "G DC",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgdTpZxzZ2QbpOVgSBCa49qEewNSQ8rsi8Y9A_W=s64",
      "userId": "09626979524344434372"
     },
     "user_tz": -480
    },
    "id": "k25S3okYHj8K",
    "outputId": "224529bf-dbe1-4c43-91a1-b8c2db9f9fdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ...\n",
      "Initialized\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Save dictionary\n",
    "'''\n",
    "dict_path = './model/Word2Vec/'\n",
    "\n",
    "print('Initializing ...')\n",
    "dictionary = corpora.Dictionary(text_pool)\n",
    "corpus = [dictionary.doc2bow(line) for line in text_pool]\n",
    "tfidf_model = models.TfidfModel(corpus, dictionary=dictionary)\n",
    "corpus_tfidf = tfidf_model[corpus]\n",
    "print(\"Initialized\")\n",
    "\n",
    "# Save dict and model\n",
    "dictionary.save(dict_path + 'dictionary.dict')\n",
    "tfidf_model.save(dict_path + 'tfidf.model')\n",
    "corpora.MmCorpus.serialize(dict_path + 'corpus.mm', corpus)\n",
    "num_features = len(dictionary.token2id.keys())\n",
    "# Similarities of sparse matrix\n",
    "index = similarities.SparseMatrixSimilarity(corpus_tfidf, num_features=num_features)\n",
    "index.save(dict_path + 'index.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T02:52:26.570909Z",
     "start_time": "2020-06-08T02:52:26.568558Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-16T03:56:29.540837Z",
     "iopub.status.busy": "2020-06-16T03:56:29.540585Z",
     "iopub.status.idle": "2020-06-16T03:56:29.543768Z",
     "shell.execute_reply": "2020-06-16T03:56:29.543292Z",
     "shell.execute_reply.started": "2020-06-16T03:56:29.540815Z"
    },
    "executionInfo": {
     "elapsed": 915,
     "status": "ok",
     "timestamp": 1591231468875,
     "user": {
      "displayName": "G DC",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgdTpZxzZ2QbpOVgSBCa49qEewNSQ8rsi8Y9A_W=s64",
      "userId": "09626979524344434372"
     },
     "user_tz": -480
    },
    "id": "KRGf6Gl7PM4u",
    "outputId": "5bf0b060-f856-45c9-e3b3-632ce66e8672"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225512\n"
     ]
    }
   ],
   "source": [
    "print(num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UFUGHf9oREd7"
   },
   "source": [
    "## Feature Engineering Preparation\n",
    "\n",
    "Corpus, Word Vectors, Models loading.\n",
    "\n",
    "* Word2Vec\n",
    "* TF-IDF\n",
    "* BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:56:33.889163Z",
     "start_time": "2020-06-09T07:55:32.676608Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-16T04:30:17.408264Z",
     "iopub.status.busy": "2020-06-16T04:30:17.408041Z",
     "iopub.status.idle": "2020-06-16T04:31:19.245836Z",
     "shell.execute_reply": "2020-06-16T04:31:19.245299Z",
     "shell.execute_reply.started": "2020-06-16T04:30:17.408242Z"
    },
    "executionInfo": {
     "elapsed": 4860,
     "status": "ok",
     "timestamp": 1591232088220,
     "user": {
      "displayName": "G DC",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgdTpZxzZ2QbpOVgSBCa49qEewNSQ8rsi8Y9A_W=s64",
      "userId": "09626979524344434372"
     },
     "user_tz": -480
    },
    "id": "qR9pIwP0R7Yv",
    "outputId": "6ff46e57-0b7d-4e99-8b96-182be070e1af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "Loaded\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, similarities, models\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from gensim.summarization.bm25 import BM25\n",
    "import numpy as np\n",
    "import math\n",
    "import collections\n",
    "from multiprocessing import cpu_count, Pool\n",
    "import Levenshtein\n",
    "import textdistance\n",
    "\n",
    "'''\n",
    "    Initialize models\n",
    "'''\n",
    "vec_model_path = './model/Word2Vec/GoogleNews-vectors-negative300.bin.gz'\n",
    "print('Loading models...')\n",
    "g_vec_model = models.KeyedVectors.load_word2vec_format(vec_model_path, binary=True)\n",
    "g_dictionary = corpora.Dictionary.load('./model/Word2Vec/dictionary.dict')\n",
    "g_tfidf_model = models.TfidfModel.load(\"./model/Word2Vec/tfidf.model\")\n",
    "g_index = similarities.SparseMatrixSimilarity.load('./model/Word2Vec/index.index')\n",
    "print('Loaded')\n",
    "\n",
    "'''\n",
    "    Pool all text from documents\n",
    "'''\n",
    "csv_folder = './data/csv/'\n",
    "document_csv_file = csv_folder + 'documents.csv'\n",
    "document_pd = pd.read_csv(document_csv_file)\n",
    "\n",
    "# pool all items from documents\n",
    "raw_text = document_pd['doc_text'].values.tolist()\n",
    "text_pool = [line.split() for line in raw_text]\n",
    "\n",
    "from collections import defaultdict\n",
    "# Remove words appear once\n",
    "word_freq = defaultdict(int)\n",
    "for line in text_pool:\n",
    "    for word in line:\n",
    "        word_freq[word] += 1\n",
    "text_pool = [[token for token in line if word_freq[token] > 1] for line in text_pool]\n",
    "\n",
    "'''\n",
    "    Load BM25 model\n",
    "'''\n",
    "from gensim.corpora.mmcorpus import MmCorpus\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "dict_path = './model/Word2Vec/'\n",
    "\n",
    "g_corpus = MmCorpus(dict_path + 'corpus.mm')\n",
    "g_bm25_model = BM25(text_pool)\n",
    "# g_vec_bm25_model = BM25(g_corpus)\n",
    "g_average_idf = sum(map(lambda k: float(g_bm25_model.idf[k]), g_bm25_model.idf.keys())) / len(g_bm25_model.idf.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:56:33.928290Z",
     "start_time": "2020-06-09T07:56:33.890674Z"
    },
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-16T04:31:19.247058Z",
     "iopub.status.busy": "2020-06-16T04:31:19.246881Z",
     "iopub.status.idle": "2020-06-16T04:31:19.286837Z",
     "shell.execute_reply": "2020-06-16T04:31:19.286278Z",
     "shell.execute_reply.started": "2020-06-16T04:31:19.247039Z"
    },
    "id": "DX1IJ4tzg08b"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Features computation utils\n",
    "'''\n",
    "def get_len(x):\n",
    "    '''\n",
    "        Length of tokens\n",
    "    '''\n",
    "    x = x.split()\n",
    "    return len(x)\n",
    "\n",
    "\n",
    "def get_token_cnt(x, y):\n",
    "    '''\n",
    "        Compute times of each token of y appeared in x\n",
    "    '''\n",
    "    x = x.split()\n",
    "    y = y.split()\n",
    "    num = 0\n",
    "    for i in y:\n",
    "        if i in x:\n",
    "            num += 1\n",
    "    return num\n",
    "\n",
    "\n",
    "def get_token_cnt_ratio(x, y):\n",
    "    x = x.split()\n",
    "    return y / len(x)\n",
    "\n",
    "\n",
    "def get_jaccard_sim(x, y):\n",
    "    '''\n",
    "        Jaccard Similarity between x & y\n",
    "    '''\n",
    "    x = set(x)\n",
    "    y = set(y)\n",
    "    return float(len(x & y) / len(x | y))\n",
    "\n",
    "\n",
    "def get_mat_cos_sim(doc, corpus):\n",
    "    '''\n",
    "        Cosine Similarity between x & y\n",
    "    '''\n",
    "    corpus = corpus.split(' ')\n",
    "    doc = doc.split(' ')\n",
    "\n",
    "    corpus_vec = [g_dictionary.doc2bow(corpus)]\n",
    "    vec = g_dictionary.doc2bow(doc)\n",
    "\n",
    "    corpus_tfidf = g_tfidf_model[corpus_vec]\n",
    "    vec_tfidf = g_tfidf_model[vec]\n",
    "\n",
    "    num_features = len(g_dictionary.token2id.keys())\n",
    "    mat_index = similarities.SparseMatrixSimilarity(corpus_tfidf, num_features=num_features)\n",
    "    sim = mat_index.get_similarities(vec_tfidf)\n",
    "\n",
    "    return sim[0]\n",
    "\n",
    "\n",
    "def get_weight_counter_and_tf_idf(x, y):\n",
    "    x = x.split()\n",
    "    y = y.split()\n",
    "    corups = x + y\n",
    "    obj = dict(collections.Counter(corups))\n",
    "    x_weight = []\n",
    "    y_weight = []\n",
    "    idfs = []\n",
    "    for key in obj.keys():\n",
    "        idf = 1\n",
    "        w = obj[key]\n",
    "        if key in x:\n",
    "            idf += 1\n",
    "            x_weight.append(w)\n",
    "        else:\n",
    "            x_weight.append(0)\n",
    "        if key in y:\n",
    "            idf += 1\n",
    "            y_weight.append(w)\n",
    "        else:\n",
    "            y_weight.append(0)\n",
    "        idfs.append(math.log(3.0 / idf) + 1)\n",
    "    return [np.array(x_weight), np.array(y_weight), np.array(x_weight) * np.array(idfs), np.array(y_weight) * np.array(idfs), np.array(list(obj.keys()))]\n",
    "\n",
    "\n",
    "def get_manhattan_distance(x, y):\n",
    "    '''\n",
    "        Manhattan distance\n",
    "    '''\n",
    "    return np.linalg.norm(x - y, ord=1)\n",
    "\n",
    "\n",
    "def get_cos_sim(x, y):\n",
    "    '''\n",
    "        Cosine similarity between vectors\n",
    "    '''\n",
    "    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "   \n",
    "def get_euclidean_sim(x, y):\n",
    "    '''\n",
    "        Euclidean similarity between vectors\n",
    "    '''\n",
    "    return np.sqrt(np.sum(x - y) ** 2)\n",
    "\n",
    "\n",
    "def get_tfidf_sim(query, doc):\n",
    "    '''\n",
    "        TF-IDF\n",
    "    '''\n",
    "    weight = list(map(lambda x, y: get_weight_counter_and_tf_idf(x, y), tqdm(query), doc))\n",
    "    x_weight_couner = []\n",
    "    y_weight_couner = []\n",
    "    x_weight_tfidf = []\n",
    "    y_weight_tfidf = []\n",
    "    words = []\n",
    "    for i in weight:\n",
    "        x_weight_couner.append(i[0])\n",
    "        y_weight_couner.append(i[1])\n",
    "        x_weight_tfidf.append(i[2])\n",
    "        y_weight_tfidf.append(i[3])\n",
    "        words.append(i[4])\n",
    "\n",
    "    mht_sim_counter = list(map(lambda x, y: get_manhattan_distance(x, y), x_weight_couner, y_weight_couner))\n",
    "    mht_sim_tfidf = list(map(lambda x, y: get_manhattan_distance(x, y), x_weight_tfidf, y_weight_tfidf))\n",
    "\n",
    "    cos_sim_counter = list(map(lambda x, y: get_cos_sim(x, y), x_weight_couner, y_weight_couner))\n",
    "    cos_sim_tfidf = list(map(lambda x, y: get_cos_sim(x, y), x_weight_tfidf, y_weight_tfidf))\n",
    "\n",
    "    euclidean_sim_counter = list(map(lambda x, y: get_euclidean_sim(x, y), x_weight_couner, y_weight_couner))\n",
    "    euclidean_sim_tfidf = list(map(lambda x, y: get_euclidean_sim(x, y), x_weight_tfidf, y_weight_tfidf))\n",
    "\n",
    "    return mht_sim_counter, mht_sim_tfidf, cos_sim_counter, cos_sim_tfidf, euclidean_sim_counter, euclidean_sim_tfidf\n",
    "\n",
    "\n",
    "def get_word_vec(x):\n",
    "    '''\n",
    "        Word2Vec\n",
    "    '''\n",
    "    vec = []\n",
    "    for word in x.split():\n",
    "        if word in g_vec_model:\n",
    "            vec.append(g_vec_model[word])\n",
    "    if len(vec) == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return np.mean(np.array(vec), axis=0)\n",
    "\n",
    "\n",
    "def get_df_grams(train_sample, values, cols):\n",
    "    def create_ngram_set(input_list, ngram_value):\n",
    "        return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
    "\n",
    "    def get_n_gram(df, values):\n",
    "        train_query = df.values\n",
    "        train_query = [[word for word in str(sen).replace(\"'\", '').split(' ')] for sen in train_query]\n",
    "        train_query_n = []\n",
    "        for input_list in train_query:\n",
    "            train_query_n_gram = set()\n",
    "            for value in range(values, values + 1):\n",
    "                train_query_n_gram = train_query_n_gram | create_ngram_set(input_list, value)\n",
    "            train_query_n.append(train_query_n_gram)\n",
    "        return train_query_n\n",
    "\n",
    "    train_query = get_n_gram(train_sample[cols[0]], values)\n",
    "    train_title = get_n_gram(train_sample[cols[1]], values)\n",
    "    sim = list(map(lambda x, y: len(x) + len(y) - 2 * len(x & y), train_query, train_title))\n",
    "    sim_number_rate = list(map(lambda x, y:   len(x & y) / len(x) if len(x) != 0 else 0, train_query, train_title))\n",
    "    return sim, sim_number_rate\n",
    "\n",
    "\n",
    "def get_token_matched_features(query, title):\n",
    "    q_list = query.split()\n",
    "    t_list = title.split()\n",
    "    set_query = set(q_list)\n",
    "    set_title = set(t_list)\n",
    "    count_words = len(set_query.union(set_title))\n",
    "\n",
    "    comwords = [word for word in t_list if word in q_list]\n",
    "    comwords_set = set(comwords)\n",
    "    unique_rate = len(comwords_set) / count_words\n",
    "\n",
    "    same_word1 = [w for w in q_list if w in t_list]\n",
    "    same_word2 = [w for w in t_list if w in q_list]\n",
    "    same_len_rate = (len(same_word1) + len(same_word2)) / \\\n",
    "        (len(q_list) + len(t_list))\n",
    "    if len(comwords) > 0:\n",
    "        com_index1 = len(comwords)\n",
    "        same_word_q = com_index1 / len(q_list)\n",
    "        same_word_t = com_index1 / len(t_list)\n",
    "\n",
    "        for word in comwords_set:\n",
    "            index_list = [i for i, x in enumerate(q_list) if x == word]\n",
    "            com_index1 += sum(index_list)\n",
    "        q_loc = com_index1 / (len(q_list) * len(comwords))\n",
    "        com_index2 = len(comwords)\n",
    "        for word in comwords_set:\n",
    "            index_list = [i for i, x in enumerate(t_list) if x == word]\n",
    "            com_index2 += sum(index_list)\n",
    "        t_loc = com_index2 / (len(t_list) * len(comwords))\n",
    "\n",
    "        same_w_set_q = len(comwords_set) / len(set_query)\n",
    "        same_w_set_t = len(comwords_set) / len(set_title)\n",
    "        word_set_rate = 2 * len(comwords_set) / \\\n",
    "            (len(set_query) + len(set_title))\n",
    "\n",
    "        com_set_query_index = len(comwords_set)\n",
    "        for word in comwords_set:\n",
    "            index_list = [i for i, x in enumerate(q_list) if x == word]\n",
    "            if len(index_list) > 0:\n",
    "                com_set_query_index += index_list[0]\n",
    "        loc_set_q = com_set_query_index / (len(q_list) * len(comwords_set))\n",
    "        com_set_title_index = len(comwords_set)\n",
    "        for word in comwords_set:\n",
    "            index_list = [i for i, x in enumerate(t_list) if x == word]\n",
    "            if len(index_list) > 0:\n",
    "                com_set_title_index += index_list[0]\n",
    "        loc_set_t = com_set_title_index / (len(t_list) * len(comwords_set))\n",
    "        set_rate = (len(comwords_set) / len(comwords))\n",
    "    else:\n",
    "        unique_rate, same_len_rate, same_word_q, same_word_t, q_loc, t_loc, same_w_set_q, same_w_set_t, word_set_rate, loc_set_q, loc_set_t, set_rate = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    return unique_rate, same_len_rate, same_word_q, same_word_t, q_loc, t_loc, same_w_set_q, same_w_set_t, word_set_rate, loc_set_q, loc_set_t, set_rate\n",
    "\n",
    "\n",
    "def get_substr_features(query, title):\n",
    "    q_list = query.split()\n",
    "    query_len = len(q_list)\n",
    "    t_list = title.split()\n",
    "    title_len = len(t_list)\n",
    "    count1 = np.zeros((query_len + 1, title_len + 1))\n",
    "    index = np.zeros((query_len + 1, title_len + 1))\n",
    "    for i in range(1, query_len + 1):\n",
    "        for j in range(1, title_len + 1):\n",
    "            if q_list[i - 1] == t_list[j - 1]:\n",
    "                count1[i][j] = count1[i - 1][j - 1] + 1\n",
    "                index[i][j] = index[i - 1][j - 1] + j\n",
    "            else:\n",
    "                count1[i][j] = 0\n",
    "                index[i][j] = 0\n",
    "    max_count1 = count1.max()\n",
    "\n",
    "    if max_count1 != 0:\n",
    "        row = int(np.where(count1 == np.max(count1))[0][0])\n",
    "        col = int(np.where(count1 == np.max(count1))[1][0])\n",
    "        mean_pos = index[row][col] / (max_count1 * title_len)\n",
    "        begin_loc = (col - max_count1 + 1) / title_len\n",
    "        rows = np.where(count1 != 0.0)[0]\n",
    "        cols = np.where(count1 != 0.0)[1]\n",
    "        total_loc = 0\n",
    "        for i in range(0, len(rows)):\n",
    "            total_loc += index[rows[i]][cols[i]]\n",
    "        density = total_loc / (query_len * title_len)\n",
    "        rate_q_len = max_count1 / query_len\n",
    "        rate_t_len = max_count1 / title_len\n",
    "    else:\n",
    "        begin_loc, mean_pos, total_loc, density, rate_q_len, rate_t_len = 0, 0, 0, 0, 0, 0\n",
    "    return max_count1, begin_loc, mean_pos, total_loc, density, rate_q_len, rate_t_len\n",
    "\n",
    "\n",
    "def get_common_words(query, title):\n",
    "    query = set(query.split())\n",
    "    title = set(title.split())\n",
    "    return len(query & title)\n",
    "\n",
    "\n",
    "def get_bm25_group(df):\n",
    "    '''\n",
    "        Build BM25 model for each query group\n",
    "    '''\n",
    "    df.columns = ['query_id', 'query_text', 'doc_text']\n",
    "    df['query_id'] = df['query_id'].fillna('always_nan')\n",
    "    query_id_group = df.groupby(['query_id'])\n",
    "    bm_list = []\n",
    "    for name, group in tqdm(query_id_group):\n",
    "        group_corpus = group['doc_text'].values.tolist()\n",
    "        group_corpus = [sentence.strip().split() for sentence in group_corpus]\n",
    "        query = group['query_text'].values[0].strip().split()\n",
    "        group_bm25_model = BM25(group_corpus)\n",
    "        # group_average_idf = sum(map(lambda k: float(group_bm25_model.idf[k]), group_bm25_model.idf.keys())) / len(group_bm25_model.idf.keys())\n",
    "        bm_score = group_bm25_model.get_scores(query) # group_average_idf)\n",
    "        bm_list.extend(bm_score)\n",
    "\n",
    "    return bm_list\n",
    "\n",
    "\n",
    "def get_bm25_overall(doc_id, query_text):\n",
    "    '''\n",
    "        Compute BM25 with model over all documents\n",
    "    '''\n",
    "    score = g_bm25_model.get_score(query_text.split(' '), document_id_2_idx[doc_id]) #g_average_idf\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xTv1YlLmhOBb"
   },
   "source": [
    "## Getting Features\n",
    "\n",
    "For each pair of query and document, we have features below:\n",
    "\n",
    "* Jaccard similarity\n",
    "* Levenshtein distance\n",
    "* Sparse matrix cosine similarity\n",
    "* TF-IDF similarities\n",
    "* Word vectors similarities\n",
    "* N-gram\n",
    "* Tokens features\n",
    "* BM25 in group and overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:56:33.947058Z",
     "start_time": "2020-06-09T07:56:33.929640Z"
    },
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-16T04:31:20.914434Z",
     "iopub.status.busy": "2020-06-16T04:31:20.914216Z",
     "iopub.status.idle": "2020-06-16T04:31:20.932649Z",
     "shell.execute_reply": "2020-06-16T04:31:20.932216Z",
     "shell.execute_reply.started": "2020-06-16T04:31:20.914414Z"
    },
    "id": "VCqE_OBiEHvO"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Compute all features\n",
    "''' \n",
    "\n",
    "def get_features(feature_data):\n",
    "    data = feature_data.copy()\n",
    "    feat_prefix = 'feat_'\n",
    "\n",
    "    # get text for each id\n",
    "    data['query_text'] = data['query_id'].apply(lambda query_id: query_map[query_id])\n",
    "    data['doc_text'] = data['doc_id'].apply(lambda doc_id: document_map[doc_id])\n",
    "    \n",
    "    data['query_len'] = data['query_text'].apply(get_len)\n",
    "    data['doc_len'] = data['doc_text'].apply(get_len)\n",
    "\n",
    "    data['query_vec'] = data['query_text'].apply(lambda x: get_word_vec(x))\n",
    "    data['doc_vec'] = data['doc_text'].apply(lambda x: get_word_vec(x))\n",
    "\n",
    "    data[feat_prefix + 'jaccard_sim'] = list(map(get_jaccard_sim, data['query_text'], data['doc_text']))\n",
    "    data[feat_prefix + 'edit_distance'] = list(map(lambda x, y: Levenshtein.distance(x, y) / (len(x) + 1), tqdm(data['query_text']), data['doc_text']))\n",
    "    data[feat_prefix + 'edit_jaro'] = list(map(lambda x, y: Levenshtein.jaro(x, y), tqdm(data['query_text']), data['doc_text']))\n",
    "    data[feat_prefix + 'edit_ratio'] = list(map(lambda x, y: Levenshtein.ratio(x, y), tqdm(data['query_text']), data['doc_text']))\n",
    "    data[feat_prefix + 'edit_jaro_winkler'] = list(map(lambda x, y: Levenshtein.jaro_winkler(x, y), tqdm(data['query_text']), data['doc_text']))\n",
    "    data[feat_prefix + 'hamming'] = list(map(lambda x, y: textdistance.Hamming(qval=None).normalized_distance(x, y), tqdm(data['query_text']), data['doc_text']))\n",
    "\n",
    "    data[feat_prefix + 'mat_cos_sim'] = list(map(lambda x, y: get_mat_cos_sim(x, y), tqdm(data['query_text']), data['doc_text']))\n",
    "\n",
    "    data[feat_prefix + 'mht_sim'], data[feat_prefix + 'tf_mht_sim'], \\\n",
    "    data[feat_prefix + 'cos_sim'], data[feat_prefix + 'tf_cos_sim'], \\\n",
    "    data[feat_prefix + 'euc_sim'], data[feat_prefix + 'tf_euc_sim'] \\\n",
    "        = get_tfidf_sim(data['query_text'], data['doc_text'])\n",
    "    \n",
    "    data[feat_prefix + 'cos_mean_word2vec'] = list(map(get_cos_sim, tqdm(data['query_vec']), data['doc_vec']))\n",
    "    data[feat_prefix + 'cos_mean_word2vec'] = data[feat_prefix + 'cos_mean_word2vec'].apply(lambda x: np.nan if np.isnan(x).any() else x)\n",
    "    data[feat_prefix + 'euc_mean_word2vec'] = list(map(get_euclidean_sim, tqdm(data['query_vec']), data['doc_vec']))\n",
    "    data[feat_prefix + 'mhd_mean_word2vec'] = list(map(get_manhattan_distance, tqdm(data['query_vec']), data['doc_vec']))\n",
    "    data[feat_prefix + 'mhd_mean_word2vec'] = list(map(lambda x, y: np.nan if np.isnan(x).any() or np.isnan(y).any() else get_manhattan_distance(x, y), tqdm(data['query_vec']), data['doc_vec']))\n",
    "    data[feat_prefix + '2_gram_sim'], data[feat_prefix + '2_sim_number_rate'] = get_df_grams(data, 2, ['query_text', 'doc_text'])\n",
    "    \n",
    "    data[feat_prefix + '3_gram_sim'], data[feat_prefix + '3_sim_number_rate'] = get_df_grams(data, 3, ['query_text', 'doc_text'])\n",
    "    \n",
    "    '''\n",
    "    data[feat_prefix + 'query_token_matched_cnt'] = list(map(get_token_cnt, data['doc_text'], data['query_text']))\n",
    "    data[feat_prefix + 'query_token_matched_cnt_ratio'] = list(map(get_token_cnt_ratio, data['query_text'], data['feat_query_token_matched_cnt']))\n",
    "    data[feat_prefix + \"ls_max_count\"], data[feat_prefix + \"ls_local_begin\"], data[feat_prefix + \"ls_local_mean\"], data[feat_prefix+\"ls_total_loc\"], data[feat_prefix + \"ls_density\"], data[feat_prefix + \"ls_rate_q_len\"], data[feat_prefix + \"ls_rate_t_len\"] = zip(*data.apply(lambda line: get_substr_features(line[\"query_text\"], line[\"doc_text\"]), axis=1))\n",
    "    data[feat_prefix + 'common_words'] = list(map(get_common_words, data['doc_text'], data['query_text']))\n",
    "    data[feat_prefix + 'common_words_rate_q'] = data[feat_prefix + 'common_words'] / data['query_len']\n",
    "    data[feat_prefix + 'common_words_rate_d'] = data[feat_prefix + 'common_words'] / data['doc_len']\n",
    "    data[feat_prefix + \"unique_rate\"], data[feat_prefix + \"same_len_rate\"], data[feat_prefix + \"same_word_q\"], data[feat_prefix + \"same_word_t\"], data[feat_prefix + \"q_loc\"], data[feat_prefix + \"t_loc\"], data[feat_prefix + \"same_w_set_q\"], data[feat_prefix + \"same_w_set_t\"], data[feat_prefix + \"word_set_rate\"], data[feat_prefix + \"loc_set_q\"], data[feat_prefix + \"loc_set_t\"], data[feat_prefix + \"set_rate\"] = zip(*data.apply(lambda line: get_token_matched_features(line[\"query_text\"], line[\"doc_text\"]), axis=1))\n",
    "    '''\n",
    "    \n",
    "    data[feat_prefix + 'bm25_group'] = get_bm25_group(data[['query_id', 'query_text', 'doc_text']])\n",
    "    data[feat_prefix + 'bm25_overall'] = list(map(get_bm25_overall, tqdm(data['doc_id']), data['query_text']))\n",
    "    \n",
    "    feat = ['query_id', 'doc_id', 'relevance']\n",
    "    for col in data.columns:\n",
    "        if col.find(feat_prefix) != -1:\n",
    "            feat.append(col)\n",
    "\n",
    "    data = data[feat]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:56:33.954200Z",
     "start_time": "2020-06-09T07:56:33.948133Z"
    },
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-16T04:31:23.454747Z",
     "iopub.status.busy": "2020-06-16T04:31:23.454502Z",
     "iopub.status.idle": "2020-06-16T04:31:23.459068Z",
     "shell.execute_reply": "2020-06-16T04:31:23.458602Z",
     "shell.execute_reply.started": "2020-06-16T04:31:23.454726Z"
    },
    "id": "dPK6SSRVU8Bd"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Documents and queries map from id to text\n",
    "'''\n",
    "document_map = dict()\n",
    "query_map = dict()\n",
    "# map doc_id to doc index in corpus\n",
    "document_id_2_idx = dict()\n",
    "\n",
    "def init_dict(query_export_file):\n",
    "    doc_idx = 0\n",
    "    for doc in zip(document_pd['doc_id'], document_pd['doc_text']):\n",
    "        document_map[doc[0]] = doc[1]\n",
    "        document_id_2_idx[doc[0]] = doc_idx\n",
    "        doc_idx += 1\n",
    "    \n",
    "    query_export_pd = pd.read_csv(query_export_file)\n",
    "    for query in zip(query_export_pd['query_id'], query_export_pd['query_text']):\n",
    "        query_map[query[0]] = query[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:56:33.971035Z",
     "start_time": "2020-06-09T07:56:33.964435Z"
    },
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-16T04:31:29.428184Z",
     "iopub.status.busy": "2020-06-16T04:31:29.427957Z",
     "iopub.status.idle": "2020-06-16T04:31:29.435473Z",
     "shell.execute_reply": "2020-06-16T04:31:29.435041Z",
     "shell.execute_reply.started": "2020-06-16T04:31:29.428163Z"
    },
    "id": "YMwnhFIvbcLf"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Interface for training dataset generation\n",
    "    * Features computation\n",
    "'''\n",
    "\n",
    "def pool_extract(data, f, chunk_size, worker=8):\n",
    "    from multiprocessing import cpu_count,Pool\n",
    "    cpu_worker = cpu_count()\n",
    "    print('CPU core: {}'.format(cpu_worker))\n",
    "    if worker == -1 or worker > cpu_worker:\n",
    "        worker = cpu_worker\n",
    "    print('Cores used: {}'.format(worker))\n",
    "    len_data = len(data)\n",
    "    start = 0\n",
    "    end = 0\n",
    "    p = Pool(worker)\n",
    "    res = []\n",
    "    while end < len_data:\n",
    "        end = start + chunk_size\n",
    "        if end > len_data:\n",
    "            end = len_data\n",
    "        rslt = p.apply_async(f, args=(data[start:end],))\n",
    "        start = end\n",
    "        res.append(rslt)\n",
    "    p.close()\n",
    "    p.join()\n",
    "    results = pd.concat([i.get() for i in res], axis=0, ignore_index=True)\n",
    "    return results\n",
    "\n",
    "def generate_features(export_file, feature_file, new_set=False):\n",
    "    feature_pd = None\n",
    "    num_worker = 8\n",
    "    if new_set:\n",
    "        export_pd = pd.read_csv(export_file)\n",
    "        feature_pd = export_pd[['doc_id', 'query_id', 'relevance']]\n",
    "    else:\n",
    "        feature_pd = pd.read_csv(feature_file)\n",
    "    CHUNK_SIZE = len(feature_pd) // num_worker + 1\n",
    "    print(f'Chunk size: {CHUNK_SIZE}, Length: {len(feature_pd)}')\n",
    "    # feature_pd = get_features(feature_pd)\n",
    "    feature_pd = pool_extract(feature_pd, get_features, CHUNK_SIZE, worker=num_worker)\n",
    "    feature_pd.to_csv(feature_file, index=False)\n",
    "    print('Features Generation Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:57:03.889255Z",
     "start_time": "2020-06-09T07:56:52.948856Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-16T04:41:34.190013Z",
     "iopub.status.busy": "2020-06-16T04:41:34.189762Z",
     "iopub.status.idle": "2020-06-16T04:45:31.660765Z",
     "shell.execute_reply": "2020-06-16T04:45:31.660246Z",
     "shell.execute_reply.started": "2020-06-16T04:41:34.189990Z"
    },
    "executionInfo": {
     "elapsed": 237000,
     "status": "ok",
     "timestamp": 1591233010355,
     "user": {
      "displayName": "G DC",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgdTpZxzZ2QbpOVgSBCa49qEewNSQ8rsi8Y9A_W=s64",
      "userId": "09626979524344434372"
     },
     "user_tz": -480
    },
    "id": "HE_SoT8128Gs",
    "outputId": "a563cc19-4d28-4f7e-ee4a-677458de8c79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size: 124148, Length: 993178\n",
      "CPU core: 8\n",
      "Cores used: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 124148/124148 [00:01<00:00, 70564.47it/s]\n",
      "100%|██████████| 124148/124148 [00:01<00:00, 69926.23it/s]\n",
      "100%|██████████| 124148/124148 [00:01<00:00, 71378.63it/s]\n",
      "100%|██████████| 124148/124148 [00:01<00:00, 70724.47it/s]\n",
      "100%|██████████| 124148/124148 [00:00<00:00, 549517.41it/s]\n",
      "100%|██████████| 124148/124148 [00:01<00:00, 67302.72it/s]\n",
      "100%|██████████| 124148/124148 [00:00<00:00, 540718.33it/s]\n",
      "\n",
      "100%|██████████| 124148/124148 [00:00<00:00, 531562.58it/s]\n",
      "100%|██████████| 124148/124148 [00:01<00:00, 70652.76it/s]\n",
      "100%|██████████| 124148/124148 [00:00<00:00, 565393.55it/s]\n",
      "100%|██████████| 124148/124148 [00:01<00:00, 70305.98it/s]\n",
      "  8%|▊         | 9726/124148 [00:00<00:01, 97239.69it/s]/s]\n",
      " 92%|█████████▏| 113992/124148 [00:00<00:00, 571557.46it/s]\n",
      "  0%|          | 0/124142 [00:00<?, ?it/s]0, 566920.15it/s]\n",
      "100%|██████████| 124148/124148 [00:00<00:00, 554841.06it/s]\n",
      "100%|██████████| 124148/124148 [00:01<00:00, 94747.66it/s]\n",
      "100%|██████████| 124148/124148 [00:01<00:00, 95860.17it/s]\n",
      "100%|██████████| 124148/124148 [00:01<00:00, 97422.86it/s]\n",
      "100%|██████████| 124148/124148 [00:01<00:00, 97918.65it/s]\n",
      "100%|██████████| 124148/124148 [00:00<00:00, 519293.15it/s]\n",
      "100%|██████████| 124148/124148 [00:01<00:00, 93494.39it/s]\n",
      "100%|██████████| 124148/124148 [00:00<00:00, 564681.70it/s]\n",
      "100%|██████████| 124148/124148 [00:00<00:00, 564747.84it/s]\n",
      "  0%|          | 0/124148 [00:00<?, ?it/s]0, 95777.23it/s]]\n",
      "100%|██████████| 124148/124148 [00:01<00:00, 95829.88it/s]\n",
      "100%|██████████| 124148/124148 [00:00<00:00, 523079.29it/s]\n",
      "100%|██████████| 124148/124148 [00:01<00:00, 96082.49it/s]\n",
      "  6%|▌         | 7122/124148 [00:00<00:03, 35834.89it/s]/s]\n",
      " 93%|█████████▎| 115442/124148 [00:00<00:00, 577194.56it/s]\n",
      "100%|██████████| 124148/124148 [00:00<00:00, 569121.67it/s]\n",
      "100%|██████████| 124148/124148 [00:00<00:00, 533615.13it/s]\n",
      "100%|██████████| 124148/124148 [00:03<00:00, 35988.28it/s]\n",
      "100%|██████████| 124148/124148 [00:03<00:00, 35926.38it/s]\n",
      "  0%|          | 0/124148 [00:00<?, ?it/s]0, 35264.80it/s]\n",
      "100%|██████████| 124148/124148 [00:03<00:00, 35194.11it/s]\n",
      "100%|██████████| 124148/124148 [00:03<00:00, 35893.89it/s]\n",
      "100%|██████████| 124148/124148 [00:03<00:00, 35538.62it/s]\n",
      "100%|██████████| 124142/124142 [00:03<00:00, 34880.27it/s]\n",
      "100%|██████████| 124148/124148 [00:03<00:00, 35016.30it/s]\n",
      "100%|██████████| 124148/124148 [02:05<00:00, 991.92it/s] \n",
      "100%|██████████| 124148/124148 [02:05<00:00, 987.79it/s] \n",
      "100%|██████████| 124148/124148 [02:05<00:00, 989.94it/s] \n",
      "100%|██████████| 124148/124148 [02:05<00:00, 989.01it/s] \n",
      "100%|██████████| 124148/124148 [02:05<00:00, 987.42it/s]\n",
      "100%|█████████▉| 123899/124148 [02:05<00:00, 977.61it/s]]\n",
      "100%|██████████| 124142/124142 [02:05<00:00, 986.80it/s] \n",
      "  8%|▊         | 10119/124148 [00:00<00:06, 16899.27it/s]\n",
      "100%|██████████| 124148/124148 [00:07<00:00, 17038.35it/s]\n",
      "100%|██████████| 124148/124148 [00:07<00:00, 17040.70it/s]\n",
      "100%|██████████| 124148/124148 [00:07<00:00, 17023.59it/s]\n",
      "100%|██████████| 124148/124148 [00:07<00:00, 16981.92it/s]\n",
      "100%|██████████| 124148/124148 [00:07<00:00, 16959.76it/s]\n",
      "100%|██████████| 124148/124148 [00:07<00:00, 16809.33it/s]\n",
      "100%|██████████| 124148/124148 [00:07<00:00, 17251.02it/s]\n",
      "100%|██████████| 124142/124142 [00:07<00:00, 16636.67it/s]\n",
      "100%|██████████| 124148/124148 [00:01<00:00, 89470.31it/s]\n",
      "100%|██████████| 124148/124148 [00:01<00:00, 86887.68it/s]\n",
      "100%|██████████| 124148/124148 [00:01<00:00, 87116.61it/s]\n",
      "100%|██████████| 124148/124148 [00:01<00:00, 87795.09it/s]\n",
      " 16%|█▋        | 20397/124148 [00:00<00:01, 102016.67it/s]\n",
      "100%|██████████| 124148/124148 [00:01<00:00, 89975.87it/s]\n",
      "100%|██████████| 124148/124148 [00:01<00:00, 88366.73it/s]\n",
      "100%|██████████| 124142/124142 [00:01<00:00, 89219.16it/s]\n",
      "100%|██████████| 124148/124148 [00:01<00:00, 100725.79it/s]\n",
      "100%|██████████| 124148/124148 [00:01<00:00, 95171.84it/s]]\n",
      " 90%|████████▉ | 111484/124148 [00:01<00:00, 101403.94it/s]\n",
      " 66%|██████▌   | 82127/124148 [00:00<00:00, 102875.00it/s]]\n",
      "100%|██████████| 124148/124148 [00:01<00:00, 101423.46it/s]\n",
      "100%|██████████| 124148/124148 [00:00<00:00, 147566.19it/s]\n",
      "  7%|▋         | 9002/124148 [00:00<00:01, 90019.01it/s]/s]\n",
      " 61%|██████    | 75745/124148 [00:00<00:00, 152042.95it/s]]\n",
      "100%|██████████| 124142/124142 [00:01<00:00, 101482.90it/s]\n",
      "100%|██████████| 124148/124148 [00:00<00:00, 149805.19it/s]\n",
      "100%|██████████| 124148/124148 [00:00<00:00, 146155.57it/s]\n",
      "100%|██████████| 124148/124148 [00:00<00:00, 149119.04it/s]\n",
      "  7%|▋         | 8912/124148 [00:00<00:01, 89115.20it/s]/s]\n",
      "100%|██████████| 124148/124148 [00:00<00:00, 147358.85it/s]\n",
      "100%|██████████| 124148/124148 [00:00<00:00, 146651.95it/s]\n",
      "100%|██████████| 124142/124142 [00:00<00:00, 149248.22it/s]\n",
      "100%|██████████| 124148/124148 [00:01<00:00, 86963.68it/s]\n",
      "100%|██████████| 124148/124148 [00:01<00:00, 87597.52it/s]\n",
      " 90%|█████████ | 111754/124148 [00:01<00:00, 84055.97it/s]\n",
      "100%|██████████| 124148/124148 [00:01<00:00, 85290.22it/s]\n",
      "100%|██████████| 124148/124148 [00:01<00:00, 85382.99it/s]\n",
      "100%|██████████| 124148/124148 [00:01<00:00, 85800.45it/s]\n",
      "100%|██████████| 124148/124148 [00:01<00:00, 86201.33it/s]\n",
      "100%|██████████| 124142/124142 [00:01<00:00, 85460.13it/s]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:269: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  0%|          | 0/3726 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:269: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  2%|▏         | 65/3726 [00:00<00:05, 643.29it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:269: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  1%|          | 39/3729 [00:00<00:09, 386.52it/s]]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:269: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:269: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  9%|▉         | 329/3726 [00:00<00:05, 668.23it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:269: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      " 10%|█         | 389/3765 [00:00<00:04, 771.47it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:269: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      " 25%|██▍       | 914/3726 [00:01<00:03, 828.99it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:269: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 3726/3726 [00:04<00:00, 750.64it/s]\n",
      "100%|██████████| 3783/3783 [00:05<00:00, 743.23it/s]6it/s]\n",
      "100%|██████████| 3729/3729 [00:05<00:00, 740.15it/s]\n",
      " 81%|████████  | 3049/3754 [00:04<00:00, 835.73it/s]51it/s]\n",
      "100%|██████████| 124148/124148 [00:00<00:00, 310971.21it/s]\n",
      "100%|██████████| 3753/3753 [00:05<00:00, 747.09it/s]\n",
      "100%|██████████| 3747/3747 [00:05<00:00, 736.21it/s]\n",
      "100%|██████████| 124148/124148 [00:00<00:00, 312334.16it/s]\n",
      "100%|██████████| 3765/3765 [00:05<00:00, 724.33it/s]\n",
      "100%|██████████| 124148/124148 [00:00<00:00, 314612.46it/s]\n",
      "100%|██████████| 124142/124142 [00:00<00:00, 305686.51it/s]\n",
      "100%|██████████| 3754/3754 [00:04<00:00, 765.73it/s]\n",
      "100%|██████████| 124148/124148 [00:00<00:00, 315537.66it/s]\n",
      "100%|██████████| 3750/3750 [00:05<00:00, 744.95it/s]2it/s]\n",
      "100%|██████████| 124148/124148 [00:00<00:00, 316156.09it/s]\n",
      "100%|██████████| 124148/124148 [00:00<00:00, 317247.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Generation Finished\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Basic settings for training features generation\n",
    "'''\n",
    "NEGATIVE_SCALE = 30 # 100 150 200\n",
    "csv_folder = './data/csv/'\n",
    "train_folder = './data/train/'\n",
    "dict_file = csv_folder + 'training.csv'\n",
    "export_file = csv_folder + 'export_training_' + str(NEGATIVE_SCALE) + '.csv'\n",
    "feature_file = train_folder + 'features_' + str(NEGATIVE_SCALE) + '.csv'\n",
    "\n",
    "init_dict(dict_file)\n",
    "generate_features(export_file, feature_file, new_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:57:06.109714Z",
     "start_time": "2020-06-09T07:57:03.890702Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-16T04:51:19.936315Z",
     "iopub.status.busy": "2020-06-16T04:51:19.936082Z",
     "iopub.status.idle": "2020-06-16T04:51:44.997925Z",
     "shell.execute_reply": "2020-06-16T04:51:44.997383Z",
     "shell.execute_reply.started": "2020-06-16T04:51:19.936292Z"
    },
    "executionInfo": {
     "elapsed": 9975,
     "status": "ok",
     "timestamp": 1591233124399,
     "user": {
      "displayName": "G DC",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgdTpZxzZ2QbpOVgSBCa49qEewNSQ8rsi8Y9A_W=s64",
      "userId": "09626979524344434372"
     },
     "user_tz": -480
    },
    "id": "UIkeJee44d9e",
    "outputId": "453aae51-202d-44bb-a855-153640f535f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size: 12354, Length: 98828\n",
      "CPU core: 8\n",
      "Cores used: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12350/12350 [00:00<00:00, 73254.40it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 70774.42it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 70920.98it/s]\n",
      "\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 69620.38it/s]\n",
      "\n",
      "  0%|          | 0/12354 [00:00<?, ?it/s], 70432.71it/s]\n",
      "100%|██████████| 12350/12350 [00:00<00:00, 593603.87it/s]\n",
      "  0%|          | 0/12354 [00:00<?, ?it/s], 589285.14it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 581292.70it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 66995.98it/s]]\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 574276.91it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 559047.46it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 90632.06it/s]\n",
      "\n",
      "  0%|          | 0/12354 [00:00<?, ?it/s], 96800.49it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 95021.80it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 90913.20it/s]\n",
      "  0%|          | 0/12354 [00:00<?, ?it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 584703.58it/s]\n",
      "100%|██████████| 12350/12350 [00:00<00:00, 575379.10it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 575591.03it/s]\n",
      "\n",
      "  0%|          | 0/12350 [00:00<?, ?it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 92203.99it/s]]\n",
      "  0%|          | 0/12354 [00:00<?, ?it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 584031.37it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 448421.34it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 561342.80it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 34687.22it/s]\n",
      "\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 35819.98it/s]\n",
      "\n",
      "  0%|          | 0/12354 [00:00<?, ?it/s], 35610.22it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 35124.24it/s]\n",
      "  0%|          | 0/12354 [00:00<?, ?it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 35152.76it/s]\n",
      "100%|██████████| 12354/12354 [00:12<00:00, 1004.49it/s]\n",
      "100%|██████████| 12354/12354 [00:12<00:00, 997.05it/s] \n",
      "100%|██████████| 12354/12354 [00:12<00:00, 986.67it/s] \n",
      "\n",
      "100%|██████████| 12354/12354 [00:12<00:00, 982.57it/s]]\n",
      "100%|██████████| 12354/12354 [00:12<00:00, 982.80it/s] \n",
      "100%|██████████| 12354/12354 [00:12<00:00, 979.90it/s] \n",
      "100%|██████████| 12354/12354 [00:12<00:00, 978.23it/s]]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 16418.99it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 16230.52it/s]\n",
      "100%|██████████| 12350/12350 [00:00<00:00, 16724.68it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 16462.65it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 16558.76it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 16638.19it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 15035.79it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 16411.40it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 86022.65it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 88162.77it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 100965.95it/s]\n",
      "100%|██████████| 12350/12350 [00:00<00:00, 87751.26it/s]\n",
      " 71%|███████   | 8800/12354 [00:00<00:00, 87995.05it/s]]\n",
      " 73%|███████▎  | 9018/12354 [00:00<00:00, 90179.66it/s]s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 97062.14it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 85855.89it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 88336.06it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 86359.71it/s]\n",
      " 71%|███████   | 8763/12354 [00:00<00:00, 87629.25it/s]s]\n",
      "100%|██████████| 12350/12350 [00:00<00:00, 101450.98it/s]\n",
      "\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 87738.95it/s]]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 100974.21it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 98945.61it/s] \n",
      "100%|██████████| 12354/12354 [00:00<00:00, 101418.49it/s]\n",
      "  0%|          | 0/12354 [00:00<?, ?it/s], 152233.27it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 86785.97it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 149073.00it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 91908.63it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 151510.03it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 97607.00it/s]]\n",
      "\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 151257.50it/s]\n",
      "  0%|          | 0/12354 [00:00<?, ?it/s], 84187.38it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 137362.14it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 86886.82it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 84406.43it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 85831.71it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 87280.95it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 79997.08it/s]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:269: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  0%|          | 0/373 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:269: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      " 16%|█▋        | 61/373 [00:00<00:00, 602.12it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:269: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      " 16%|█▌        | 58/373 [00:00<00:00, 576.40it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:269: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  0%|          | 0/377 [00:00<?, ?it/s]54.81it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:269: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      " 34%|███▍      | 128/373 [00:00<00:00, 608.07it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:269: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:269: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      " 44%|████▍     | 166/379 [00:00<00:00, 835.34it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:269: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      " 46%|████▌     | 171/374 [00:00<00:00, 863.31it/s]\n",
      "100%|██████████| 373/373 [00:00<00:00, 762.37it/s]\n",
      " 82%|████████▏ | 310/377 [00:00<00:00, 764.99it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 221702.08it/s]\n",
      " 68%|██████▊   | 256/374 [00:00<00:00, 858.26it/s]22it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 212500.03it/s]\n",
      "100%|██████████| 377/377 [00:00<00:00, 760.58it/s]\n",
      "100%|██████████| 378/378 [00:00<00:00, 826.67it/s]\n",
      "  0%|          | 0/12354 [00:00<?, ?it/s], 223539.39it/s]\n",
      "100%|██████████| 377/377 [00:00<00:00, 754.57it/s]\n",
      "  0%|          | 0/12354 [00:00<?, ?it/s]6.69it/s]\n",
      "100%|██████████| 376/376 [00:00<00:00, 801.08it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 219814.50it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 218270.95it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 221413.14it/s]\n",
      "100%|██████████| 12354/12354 [00:00<00:00, 219616.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Generation Finished\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Basic settings for validation features generation\n",
    "'''\n",
    "NEGATIVE_SCALE = 30\n",
    "csv_folder = './data/csv/'\n",
    "train_folder = './data/train/'\n",
    "dict_file = csv_folder + 'validation.csv'\n",
    "export_file = csv_folder + 'export_validation_' + str(NEGATIVE_SCALE) + '.csv'\n",
    "feature_file = train_folder + 'validation_' + str(NEGATIVE_SCALE) + '.csv'\n",
    "\n",
    "init_dict(dict_file)\n",
    "generate_features(export_file, feature_file, new_set=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNv+Acx12fE/zBpK/XSvO3+",
   "collapsed_sections": [],
   "mount_file_id": "1M93iSl5oVlIFBwFrIHWCuYAQ966HHtgt",
   "name": "IR_Get_Features",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
